{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "This notebook is a demo for people who are interested in further research on this project or who are trying to reproduce the results. Python3 is required for the code to run properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "To install the requirements, run the following code in the terminal.\n",
    "`pip3 install -r requirements.txt`\n",
    "This command would install all dependencies required for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "First we need to import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants Definition\n",
    "\n",
    "Here the constants are defined to make the requests with the Pageview API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "# Assuming roughly 2ms latency on the API and network\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "PROJECT_ROOT_DIR = Path(os.getcwd()).absolute().parent\n",
    "article_df = pd.read_csv(os.path.join(\n",
    "    PROJECT_ROOT_DIR, 'data', 'data_raw', 'dinosaur_genera.cleaned.SEPT.2022.csv'))\n",
    "ARTICLE_TITLES = article_df['name']\n",
    "ARTICLE_TITLES[0] = '\"Coelosaurus\" antiquus'\n",
    "ARTICLE_URLS = article_df['url']\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"20150101\",\n",
    "    \"end\":         \"20221001\"    # this is likely the wrong end date\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start anything with the data, we need to decide what is the start date and end date of this project. They should be in the format of \"%Y%m%d%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '20150701'\n",
    "end_date = '20221001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "To obtain the JSON files in `/data/data_clean/`, we need to process the files individually.\n",
    "The following is a function used to collect response for a given article from a certain start date to a certain end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageviews_per_article(article_title=None, start_date=None,\n",
    "                                  end_date=None,\n",
    "                                  endpoint_url=API_REQUEST_PAGEVIEWS_ENDPOINT,\n",
    "                                  endpoint_params=API_REQUEST_PER_ARTICLE_PARAMS,\n",
    "                                  request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title:\n",
    "        return None\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ', '_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "\n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url + endpoint_params.format(**request_template)\n",
    "\n",
    "    # Update the dates if necessary\n",
    "    if start_date:\n",
    "        ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['start'] = start_date\n",
    "\n",
    "    if end_date:\n",
    "        ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['end'] = end_date\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate monthly desktop traffic data\n",
    "\n",
    "For each article title, we try to get a response from the Pageview server and update to our own json. There is a chance that the response is empty/invalid. For those error responses, we collect them to `error_articles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1423/1423 [07:37<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "monthly_desktop_json = {}\n",
    "error_articles = {\"titles\": []}\n",
    "\n",
    "for article_title in tqdm.tqdm(ARTICLE_TITLES):\n",
    "    json_response = request_pageviews_per_article(article_title=article_title,\n",
    "                                                    start_date=start_date, end_date=end_date)\n",
    "\n",
    "    try:\n",
    "        monthly_desktop_json.update({article_title: json_response['items']})\n",
    "    except KeyError:\n",
    "        print(f'{article_title} has error. Please check.')\n",
    "        error_articles['titles'].append(article_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error articles are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save it to a JSON file in `/data/data_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates output\n",
    "if not os.path.exists(os.path.join(PROJECT_ROOT_DIR, 'data')):\n",
    "    os.mkdir('data')\n",
    "\n",
    "with open(os.path.join(PROJECT_ROOT_DIR, \"data\", \"data_clean\", f\"dino_monthly_desktop_<start{start_date[:6]}>-<end{end_date[:6]}>.json\"), \"w\") as f:\n",
    "    json.dump(monthly_desktop_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate monthly mobile traffic data\n",
    "\n",
    "Similar to desktop traffic data, we are collecting each article's monthly mobile access data. The API call accesses mobile-web and mobile-app individually, so we need to combine the view counts before adding to our own json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_mobile_json = {}\n",
    "error_articles = {\"titles\": []}\n",
    "\n",
    "for article_title in tqdm.tqdm(ARTICLE_TITLES):\n",
    "    mobile_req_params = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE.copy()\n",
    "\n",
    "    # Mobile-app response\n",
    "    mobile_req_params['access'] = 'mobile-app'\n",
    "    app_json_response = request_pageviews_per_article(article_title=article_title,\n",
    "                                                        start_date=start_date, end_date=end_date, request_template=mobile_req_params)\n",
    "\n",
    "    # Mobile-web response\n",
    "    mobile_req_params['access'] = 'mobile-web'\n",
    "    web_json_response = request_pageviews_per_article(article_title=article_title,\n",
    "                                                        start_date=start_date, end_date=end_date, request_template=mobile_req_params)\n",
    "\n",
    "    try:\n",
    "        # Combine the web and app results\n",
    "        comb_json_response = [{\n",
    "            'project': web['project'],\n",
    "            'article': web['article'],\n",
    "            'granularity': web['granularity'],\n",
    "            'timestamp': web['timestamp'],\n",
    "            'access': 'mobile',\n",
    "            'agent': 'user',\n",
    "            'views': web['views'] + app['views']\n",
    "        } for web, app in zip(web_json_response['items'], app_json_response['items'])]\n",
    "\n",
    "        # Add to result json\n",
    "        monthly_mobile_json.update({article_title: comb_json_response})\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'{article_title} has error. Please check.')\n",
    "        error_articles['titles'].append(article_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an output in `/data/data_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates output\n",
    "if not os.path.exists(os.path.join(PROJECT_ROOT_DIR, 'data')):\n",
    "    os.mkdir('data')\n",
    "\n",
    "with open(os.path.join(PROJECT_ROOT_DIR, \"data\", \"data_clean\", f\"dino_monthly_mobile_<start{start_date[:6]}>-<end{end_date[:6]}>.json\"), \"w\") as f:\n",
    "    json.dump(monthly_mobile_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate monthly cumulative access\n",
    "\n",
    "We also need to collect all the monthly cumulative access counts for both desktop and mobile accesses for each title. To do this, we need to get three responses: mobile app, mobile web and desktop, and add them up for each month. We also need to accumulate the view counts before saving it to our own json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_accumu_json = {}\n",
    "error_articles = {\"titles\": []}\n",
    "\n",
    "for article_title in tqdm.tqdm(ARTICLE_TITLES):\n",
    "    mobile_req_params = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE.copy()\n",
    "\n",
    "    # Mobile app response\n",
    "    mobile_req_params['access'] = 'mobile-app'\n",
    "    app_json_response = request_pageviews_per_article(article_title=article_title,\n",
    "        start_date=start_date, end_date=end_date, request_template=mobile_req_params)\n",
    "\n",
    "    # Mobile web response\n",
    "    mobile_req_params['access'] = 'mobile-web'\n",
    "    web_json_response = request_pageviews_per_article(article_title=article_title,\n",
    "        start_date=start_date, end_date=end_date, request_template=mobile_req_params)\n",
    "\n",
    "    # Desktop response\n",
    "    desktop_json_response = request_pageviews_per_article(article_title=article_title,\n",
    "        start_date=start_date, end_date=end_date, request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE)\n",
    "\n",
    "    try:\n",
    "        # Combine results from all three responses\n",
    "        comb_json_response = [{\n",
    "            'project': web['project'],\n",
    "            'article': web['article'],\n",
    "            'granularity': web['granularity'],\n",
    "            'timestamp': web['timestamp'],\n",
    "            'agent': 'user',\n",
    "            'views': web['views'] + app['views'] + desktop['views']\n",
    "        } for web, app, desktop in zip(web_json_response['items'], app_json_response['items'], desktop_json_response['items'])]\n",
    "\n",
    "        # Accumulate the views\n",
    "        for i, comb in enumerate(comb_json_response):\n",
    "            if i > 0:\n",
    "                prev_comb = comb_json_response[i - 1]\n",
    "                comb['views'] += prev_comb['views']\n",
    "\n",
    "        monthly_accumu_json.update({article_title: comb_json_response})\n",
    "    except KeyError:\n",
    "        print(f'{article_title} has error. Please check.')\n",
    "        error_articles['titles'].append(article_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates output at `/data/data_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates output\n",
    "if not os.path.exists(os.path.join(PROJECT_ROOT_DIR, 'data')):\n",
    "    os.mkdir('data')\n",
    "\n",
    "with open(os.path.join(PROJECT_ROOT_DIR, \"data\", \"data_clean\", f\"dino_monthly_cumulative_<start{start_date[:6]}>-<end{end_date[:6]}>.json\"), \"w\") as f:\n",
    "    json.dump(monthly_accumu_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "After we have generated the json data files, we could use them to produce visualizations for further data analysis. The following three visualizations are created: <br>\n",
    "1. Maximum Average and Minimum Average <br>\n",
    "    This graph contains time series for articles with the highest average page requests and lowest average page requests for both desktop and mobile access. This is located in `img/max_min_avg.png`. <br>\n",
    "2. Top 10 Peak Page Views <br>\n",
    "    This graph contains time series for the top 10 article pages by peak page views over the entire time by access type. This is located in `img/top10peak.png`. <br>\n",
    "3. Fewest Months of Data <br>\n",
    "    This graph shows the pages with the fewest months of available data. Some of them have only one month of data. The created graph is located in `img/top10peak.png`.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_ROOT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROJECT_ROOT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_clean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdino_monthly_desktop_<start201501>-<end202210>.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     desktop_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROJECT_ROOT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_clean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdino_monthly_mobile_<start201501>-<end202210>.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROJECT_ROOT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data into dict\n",
    "with open(os.path.join(PROJECT_ROOT_DIR, 'data', 'data_clean', 'dino_monthly_desktop_<start201501>-<end202210>.json'), 'r') as f:\n",
    "    desktop_dict = json.load(f)\n",
    "\n",
    "with open(os.path.join(PROJECT_ROOT_DIR, 'data', 'data_clean', 'dino_monthly_mobile_<start201501>-<end202210>.json'), 'r') as f:\n",
    "    mobile_dict = json.load(f)\n",
    "\n",
    "\n",
    "# Rearrange the data and filter out unneeded attributes. Keep only views and time\n",
    "desktop_views = {\n",
    "    article: {\n",
    "        'views':[month['views'] for month in desktop_dict[article]],\n",
    "        'months': [month['timestamp'][:-2] for month in desktop_dict[article]] \n",
    "    } for article in desktop_dict\n",
    "}\n",
    "\n",
    "mobile_views = {\n",
    "    article: {\n",
    "        'views':[month['views'] for month in mobile_dict[article]],\n",
    "        'months': [month['timestamp'][:8] for month in mobile_dict[article]] \n",
    "    } for article in mobile_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Average and Minimum Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine which article has the max and min average monthly counts for desktop access and mobile access. The max, min are stored in tuple format, where `tuple[0]` is the article name, `tuple[1]` is the average count, and `tuple[2]` is a list containing the timespan of this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize max and min for desktop and mobile access\n",
    "max_desk = ('', -float('inf'), [])\n",
    "min_desk = ('', float('inf'), [])\n",
    "max_mob = ('', -float('inf'), [])\n",
    "min_mob = ('', float('inf'), [])\n",
    "\n",
    "# Get the max and min for desktop\n",
    "for article, param in desktop_views.items():\n",
    "    article_avg = np.mean(param['views'])\n",
    "    months = param['months']\n",
    "    if article_avg > max_desk[1]:\n",
    "        max_desk = (article, article_avg, months)\n",
    "    if article_avg < min_desk[1]:\n",
    "        min_desk = (article, article_avg, months)\n",
    "\n",
    "# Get the max and min for mobile\n",
    "for article, param in mobile_views.items():\n",
    "    article_avg = np.mean(param['views'])\n",
    "    months = param['months']\n",
    "    if article_avg > max_mob[1]:\n",
    "        max_mob = (article, article_avg, months)\n",
    "    if article_avg < min_mob[1]:\n",
    "        min_mob = (article, article_avg, months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the graph with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
    "\n",
    "sns.lineplot(x=pd.to_datetime(max_desk[2], format='%Y%m%d'), y=desktop_views[max_desk[0]]['views'], label=f'Desktop Max - {max_desk[0]}')\n",
    "sns.lineplot(x=pd.to_datetime(min_desk[2], format='%Y%m%d'), y=desktop_views[min_desk[0]]['views'], label=f'Desktop Min - {min_desk[0]}')\n",
    "sns.lineplot(x=pd.to_datetime(max_mob[2], format='%Y%m%d'), y=mobile_views[max_mob[0]]['views'], label=f'Mobile Max - {max_mob[0]}')\n",
    "sns.lineplot(x=pd.to_datetime(min_mob[2], format='%Y%m%d'), y=mobile_views[min_mob[0]]['views'], label=f'Mobile Max - {min_mob[0]}')\n",
    "\n",
    "# Graph config\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Max and Min average for desktop and mobile\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Views')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Peak Page Views\n",
    "\n",
    "For this graph, we need to get a peak value for all of the articles first, sort the peak views and get the top 10 for both desktop and mobile access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the data structure: find the peak value for each article\n",
    "desktop_max = {\n",
    "    article: {\n",
    "        'views': desktop_views[article]['views'][np.argmax(desktop_views[article]['views'])]\n",
    "    } for article in desktop_dict\n",
    "}\n",
    "\n",
    "mobile_max = {\n",
    "    article: {\n",
    "        'views': mobile_views[article]['views'][np.argmax(mobile_views[article]['views'])]\n",
    "    } for article in mobile_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the articles based on the peak value, then select the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_desktop = [(article, desktop_max[article]['views']) for article in desktop_max]\n",
    "top_10_mobile = [(article, mobile_max[article]['views']) for article in mobile_max]\n",
    "\n",
    "top_10_desktop = sorted(top_10_desktop, key=lambda x:x[1], reverse=True)[:10]\n",
    "top_10_mobile = sorted(top_10_mobile, key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
    "for article, _, _ in top_10_desktop:\n",
    "    ax = sns.lineplot(x=pd.to_datetime(desktop_views[article]['months'], format='%Y%m%d'), y=desktop_views[article]['views'], linewidth=1, label=f'D: {article}')\n",
    "\n",
    "for article, _, _ in top_10_mobile:\n",
    "    ax = sns.lineplot(x=pd.to_datetime(mobile_views[article]['months'], format='%Y%m%d'), y=mobile_views[article]['views'], linewidth=1, linestyle='--', label=f'M: {article}')\n",
    "\n",
    "# Graph config\n",
    "plt.yscale('log')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 for desktop and mobile\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Views')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fewest Months of Data \n",
    "This visualization finds the top 10 articles with the least amount of data in terms of fewest months. First we need to get the length of the vews from the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the the articles and length of views from the data files\n",
    "desktop_len = [(article, len(desktop_views[article]['views'])) for article in desktop_dict]\n",
    "mobile_len = [(article, len(mobile_views[article]['views'])) for article in mobile_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sort the articles by length ascending and select the first ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_10_desktop_len = sorted(desktop_len, key=lambda x:x[1])[:10]\n",
    "min_10_mobile_len = sorted(mobile_len, key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph: there would be 20 lines in total, ten for desktop access and ten for mobile access. Most of them have only one month of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
    "for article, _ in min_10_desktop_len:\n",
    "    sns.lineplot(x=pd.to_datetime(desktop_views[article]['months'], format='%Y%m%d'), y=desktop_views[article]['views'], linewidth=1, label=f'Desktop: {article}')\n",
    "\n",
    "for article, _ in min_10_mobile_len:\n",
    "    sns.lineplot(x=pd.to_datetime(mobile_views[article]['months'], format='%Y%m%d'), y=mobile_views[article]['views'], linewidth=1, linestyle='--', label=f'Mobile: {article}')\n",
    "\n",
    "# Graph config\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Fewest month of data for desktop and mobile\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Views')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "571a56e4d6b66236f44178d3849f7db975d463e3b40ed3d72e32ea01a686ced8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
